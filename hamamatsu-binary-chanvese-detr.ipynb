{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T22:09:15.634576Z",
     "iopub.status.busy": "2023-04-11T22:09:15.634059Z",
     "iopub.status.idle": "2023-04-11T22:09:23.698291Z",
     "shell.execute_reply": "2023-04-11T22:09:23.697054Z",
     "shell.execute_reply.started": "2023-04-11T22:09:15.634516Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:34:53.849282Z",
     "iopub.status.busy": "2023-04-12T22:34:53.848867Z",
     "iopub.status.idle": "2023-04-12T22:35:58.939800Z",
     "shell.execute_reply": "2023-04-12T22:35:58.938557Z",
     "shell.execute_reply.started": "2023-04-12T22:34:53.849220Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -i https://test.pypi.org/simple/ supervision==0.3.0  ## Install necessary packages\n",
    "!pip install -q transformers\n",
    "!pip install -q pytorch-lightning\n",
    "#!pip install -q roboflow\n",
    "!pip install -q timm\n",
    "\n",
    "\n",
    "import torch\n",
    "import supervision as sv\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])  ## For cuda and torch versions\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "\n",
    "#import roboflow\n",
    "import supervision\n",
    "import transformers\n",
    "import pytorch_lightning\n",
    "\n",
    "print(\n",
    "    #\"roboflow:\", roboflow.__version__, \n",
    "    \"; supervision:\", supervision.__version__, \n",
    "    \"; transformers:\", transformers.__version__, \n",
    "    \"; pytorch_lightning:\", pytorch_lightning.__version__\n",
    ")\n",
    "\n",
    "import torch\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "\n",
    "\n",
    "# settings\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') ## GPU Enabled\n",
    "CHECKPOINT = 'facebook/detr-resnet-50'\n",
    "CONFIDENCE_TRESHOLD = 0.5\n",
    "IOU_TRESHOLD = 0.8\n",
    "\n",
    "image_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)\n",
    "model = DetrForObjectDetection.from_pretrained(CHECKPOINT)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:35:58.942877Z",
     "iopub.status.busy": "2023-04-12T22:35:58.942460Z",
     "iopub.status.idle": "2023-04-12T22:35:58.949990Z",
     "shell.execute_reply": "2023-04-12T22:35:58.948975Z",
     "shell.execute_reply.started": "2023-04-12T22:35:58.942837Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:35:58.952112Z",
     "iopub.status.busy": "2023-04-12T22:35:58.951469Z",
     "iopub.status.idle": "2023-04-12T22:36:01.293585Z",
     "shell.execute_reply": "2023-04-12T22:36:01.292272Z",
     "shell.execute_reply.started": "2023-04-12T22:35:58.952059Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/cocodataset/cocoapi/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:36:01.298766Z",
     "iopub.status.busy": "2023-04-12T22:36:01.298443Z",
     "iopub.status.idle": "2023-04-12T22:36:17.253234Z",
     "shell.execute_reply": "2023-04-12T22:36:17.252022Z",
     "shell.execute_reply.started": "2023-04-12T22:36:01.298732Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd cocoapi\n",
    "%cd PythonAPI\n",
    "!make\n",
    "!python setup.py  install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:36:17.256373Z",
     "iopub.status.busy": "2023-04-12T22:36:17.254903Z",
     "iopub.status.idle": "2023-04-12T22:36:17.299470Z",
     "shell.execute_reply": "2023-04-12T22:36:17.298142Z",
     "shell.execute_reply.started": "2023-04-12T22:36:17.256327Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "\n",
    "location = \"/kaggle/input/chanvese-hamamamtsu-ap-coco\"   ## This is the location of dataset on kaggle\n",
    "# settings\n",
    "ANNOTATION_FILE_NAME = \"_annotations.coco.json\"\n",
    "TRAIN_DIRECTORY = os.path.join(location, \"train\")\n",
    "VAL_DIRECTORY = os.path.join(location, \"valid\")\n",
    "TEST_DIRECTORY = os.path.join(location, \"test\")\n",
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_directory_path: str, \n",
    "        image_processor, \n",
    "        train: bool = True\n",
    "    ):\n",
    "        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n",
    "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, annotations = super(CocoDetection, self).__getitem__(idx)        \n",
    "        image_id = self.ids[idx]\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")  ## Encoding according to annotations\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()  ## These are the actual\n",
    "        target = encoding[\"labels\"][0]  ## These are the annotated images\n",
    "\n",
    "        return pixel_values, target\n",
    "    \n",
    "    \n",
    "TRAIN_DATASET = CocoDetection(        ## Train dataset prepared\n",
    "    image_directory_path=TRAIN_DIRECTORY, \n",
    "    image_processor=image_processor, \n",
    "    train=True)\n",
    "VAL_DATASET = CocoDetection(    ## Validation dataset prepared\n",
    "    image_directory_path=VAL_DIRECTORY, \n",
    "    image_processor=image_processor, \n",
    "    train=False)\n",
    "TEST_DATASET = CocoDetection(   ## Test dataset prepared\n",
    "    image_directory_path=TEST_DIRECTORY, \n",
    "    image_processor=image_processor, \n",
    "    train=False)\n",
    "\n",
    "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
    "print(\"Number of validation examples:\", len(VAL_DATASET))\n",
    "print(\"Number of test examples:\", len(TEST_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:36:17.301682Z",
     "iopub.status.busy": "2023-04-12T22:36:17.301036Z",
     "iopub.status.idle": "2023-04-12T22:36:17.993006Z",
     "shell.execute_reply": "2023-04-12T22:36:17.992132Z",
     "shell.execute_reply.started": "2023-04-12T22:36:17.301642Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# select random image\n",
    "image_ids = TRAIN_DATASET.coco.getImgIds()   ## random image generation\n",
    "image_id = random.choice(image_ids)\n",
    "print('Image #{}'.format(image_id))\n",
    "\n",
    "# load image and annotatons \n",
    "image = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n",
    "annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n",
    "image_path = os.path.join(TRAIN_DATASET.root, image['file_name'])\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# annotate\n",
    "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
    "\n",
    "# we will use id2label function for training\n",
    "categories = TRAIN_DATASET.coco.cats\n",
    "id2label = {k: v['name'] for k,v in categories.items()}    ## Labels need to be given for each class\n",
    "\n",
    "labels = [\n",
    "    f\"{id2label[class_id]}\" \n",
    "    for _, _, class_id, _ \n",
    "    in detections\n",
    "]\n",
    "\n",
    "box_annotator = sv.BoxAnnotator()      ## This will annotate the images using the detections\n",
    "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
    "\n",
    "%matplotlib inline  \n",
    "sv.show_frame_in_notebook(image, (16, 16))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:36:17.994493Z",
     "iopub.status.busy": "2023-04-12T22:36:17.994172Z",
     "iopub.status.idle": "2023-04-12T22:36:18.003726Z",
     "shell.execute_reply": "2023-04-12T22:36:18.002731Z",
     "shell.execute_reply.started": "2023-04-12T22:36:17.994462Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # DETR authors employ various image sizes during training, making it not possible \n",
    "    # to directly batch together images. Hence they pad the images to the biggest \n",
    "    # resolution in a given batch, and create a corresponding binary pixel_mask \n",
    "    # which indicates which pixels are real/which are padding\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'pixel_mask': encoding['pixel_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=8, shuffle=True)\n",
    "VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=8)\n",
    "TEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:36:18.006515Z",
     "iopub.status.busy": "2023-04-12T22:36:18.005732Z",
     "iopub.status.idle": "2023-04-12T22:36:18.022003Z",
     "shell.execute_reply": "2023-04-12T22:36:18.020955Z",
     "shell.execute_reply.started": "2023-04-12T22:36:18.006478Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import DetrForObjectDetection\n",
    "import torch\n",
    "\n",
    "## Following is the complete DETR model and what'll happen in each step\n",
    "class Detr(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, lr, lr_backbone, weight_decay):\n",
    "        super().__init__()\n",
    "        self.model = DetrForObjectDetection.from_pretrained(\n",
    "            pretrained_model_name_or_path=CHECKPOINT, \n",
    "            num_labels=len(id2label),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step, and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation/loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # DETR authors decided to use different learning rate for backbone\n",
    "        # you can learn more about it here: \n",
    "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L22-L23\n",
    "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L131-L139\n",
    "        param_dicts = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                \"lr\": self.lr_backbone,\n",
    "            },\n",
    "        ]\n",
    "        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return TRAIN_DATALOADER\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return VAL_DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:36:18.023828Z",
     "iopub.status.busy": "2023-04-12T22:36:18.023421Z",
     "iopub.status.idle": "2023-04-12T22:36:24.611609Z",
     "shell.execute_reply": "2023-04-12T22:36:24.610182Z",
     "shell.execute_reply.started": "2023-04-12T22:36:18.023791Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:36:24.616101Z",
     "iopub.status.busy": "2023-04-12T22:36:24.615740Z",
     "iopub.status.idle": "2023-04-12T22:36:53.966991Z",
     "shell.execute_reply": "2023-04-12T22:36:53.965573Z",
     "shell.execute_reply.started": "2023-04-12T22:36:24.616046Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)  ## Hyperparameters set\n",
    "\n",
    "batch = next(iter(TRAIN_DATALOADER))\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:36:53.974261Z",
     "iopub.status.busy": "2023-04-12T22:36:53.971838Z",
     "iopub.status.idle": "2023-04-12T22:36:53.985890Z",
     "shell.execute_reply": "2023-04-12T22:36:53.985005Z",
     "shell.execute_reply.started": "2023-04-12T22:36:53.974216Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs.logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T22:36:53.989536Z",
     "iopub.status.busy": "2023-04-12T22:36:53.987471Z",
     "iopub.status.idle": "2023-04-13T00:41:24.546242Z",
     "shell.execute_reply": "2023-04-13T00:41:24.545201Z",
     "shell.execute_reply.started": "2023-04-12T22:36:53.989449Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "# settings\n",
    "MAX_EPOCHS = 70\n",
    "\n",
    "# pytorch_lightning < 2.0.0\n",
    "# trainer = Trainer(gpus=1, max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
    "\n",
    "# pytorch_lightning >= 2.0.0\n",
    "trainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T00:41:24.548155Z",
     "iopub.status.busy": "2023-04-13T00:41:24.547784Z",
     "iopub.status.idle": "2023-04-13T00:41:25.528989Z",
     "shell.execute_reply": "2023-04-13T00:41:25.527867Z",
     "shell.execute_reply.started": "2023-04-13T00:41:24.548116Z"
    }
   },
   "outputs": [],
   "source": [
    "path1 = \"/kaggle/working/Hamamatsu_Binary_Chanvese_DETR.pt\"   ## Model saved\n",
    "torch.save(model, path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
