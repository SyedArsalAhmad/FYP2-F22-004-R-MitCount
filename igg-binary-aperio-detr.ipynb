{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"import torch\n","metadata":{"execution":{"iopub.status.busy":"2023-04-11T21:25:00.231592Z","iopub.execute_input":"2023-04-11T21:25:00.232002Z","iopub.status.idle":"2023-04-11T21:25:00.237774Z","shell.execute_reply.started":"2023-04-11T21:25:00.231966Z","shell.execute_reply":"2023-04-11T21:25:00.236295Z"}}},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -i https://test.pypi.org/simple/ supervision==0.3.0\n!pip install -q transformers\n!pip install -q pytorch-lightning\n#!pip install -q roboflow\n!pip install -q timm\n\n\nimport torch\nimport supervision as sv\n!nvcc --version\nTORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\nCUDA_VERSION = torch.__version__.split(\"+\")[-1]\nprint(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n\n#import roboflow\nimport supervision\nimport transformers\nimport pytorch_lightning\n\nprint(\n    #\"roboflow:\", roboflow.__version__, \n    \"; supervision:\", supervision.__version__, \n    \"; transformers:\", transformers.__version__, \n    \"; pytorch_lightning:\", pytorch_lightning.__version__\n)\n\nimport torch\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\n\n\n# settings\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nCHECKPOINT = 'facebook/detr-resnet-50'\nCONFIDENCE_TRESHOLD = 0.5\nIOU_TRESHOLD = 0.8\n\nimage_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)\nmodel = DetrForObjectDetection.from_pretrained(CHECKPOINT)\nmodel.to(DEVICE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/cocodataset/cocoapi/\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd cocoapi\n%cd PythonAPI\n!make\n!python setup.py  install","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torchvision\n\nlocation = \"/kaggle/input/igg-binary-aperio-coco\"\n# settings\nANNOTATION_FILE_NAME = \"_annotations.coco.json\"\nTRAIN_DIRECTORY = os.path.join(location, \"train\")\nVAL_DIRECTORY = os.path.join(location, \"valid\")\nTEST_DIRECTORY = os.path.join(location, \"test\")\n\n\nclass CocoDetection(torchvision.datasets.CocoDetection):\n    def __init__(\n        self, \n        image_directory_path: str, \n        image_processor, \n        train: bool = True\n    ):\n        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n        self.image_processor = image_processor\n\n    def __getitem__(self, idx):\n        images, annotations = super(CocoDetection, self).__getitem__(idx)        \n        image_id = self.ids[idx]\n        annotations = {'image_id': image_id, 'annotations': annotations}\n        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n        pixel_values = encoding[\"pixel_values\"].squeeze()\n        target = encoding[\"labels\"][0]\n\n        return pixel_values, target\n    \n    \nTRAIN_DATASET = CocoDetection(\n    image_directory_path=TRAIN_DIRECTORY, \n    image_processor=image_processor, \n    train=True)\nVAL_DATASET = CocoDetection(\n    image_directory_path=VAL_DIRECTORY, \n    image_processor=image_processor, \n    train=False)\nTEST_DATASET = CocoDetection(\n    image_directory_path=TEST_DIRECTORY, \n    image_processor=image_processor, \n    train=False)\n\nprint(\"Number of training examples:\", len(TRAIN_DATASET))\nprint(\"Number of validation examples:\", len(VAL_DATASET))\nprint(\"Number of test examples:\", len(TEST_DATASET))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport cv2\nimport numpy as np\n\n\n# select random image\nimage_ids = TRAIN_DATASET.coco.getImgIds()\nimage_id = random.choice(image_ids)\nprint('Image #{}'.format(image_id))\n\n# load image and annotatons \nimage = TRAIN_DATASET.coco.loadImgs(image_id)[0]\nannotations = TRAIN_DATASET.coco.imgToAnns[image_id]\nimage_path = os.path.join(TRAIN_DATASET.root, image['file_name'])\nimage = cv2.imread(image_path)\n\n# annotate\ndetections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n\n# we will use id2label function for training\ncategories = TRAIN_DATASET.coco.cats\nid2label = {k: v['name'] for k,v in categories.items()}\n\nlabels = [\n    f\"{id2label[class_id]}\" \n    for _, _, class_id, _ \n    in detections\n]\n\nbox_annotator = sv.BoxAnnotator()\nframe = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n\n%matplotlib inline  \nsv.show_frame_in_notebook(image, (16, 16))\n     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n    # DETR authors employ various image sizes during training, making it not possible \n    # to directly batch together images. Hence they pad the images to the biggest \n    # resolution in a given batch, and create a corresponding binary pixel_mask \n    # which indicates which pixels are real/which are padding\n    pixel_values = [item[0] for item in batch]\n    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n    labels = [item[1] for item in batch]\n    return {\n        'pixel_values': encoding['pixel_values'],\n        'pixel_mask': encoding['pixel_mask'],\n        'labels': labels\n    }\n\nTRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=8, shuffle=True)\nVAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=8)\nTEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom transformers import DetrForObjectDetection\nimport torch\n\n\nclass Detr(pl.LightningModule):\n\n    def __init__(self, lr, lr_backbone, weight_decay):\n        super().__init__()\n        self.model = DetrForObjectDetection.from_pretrained(\n            pretrained_model_name_or_path=CHECKPOINT, \n            num_labels=len(id2label),\n            ignore_mismatched_sizes=True\n        )\n        \n        self.lr = lr\n        self.lr_backbone = lr_backbone\n        self.weight_decay = weight_decay\n\n    def forward(self, pixel_values, pixel_mask):\n        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n\n    def common_step(self, batch, batch_idx):\n        pixel_values = batch[\"pixel_values\"]\n        pixel_mask = batch[\"pixel_mask\"]\n        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n\n        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n\n        loss = outputs.loss\n        loss_dict = outputs.loss_dict\n\n        return loss, loss_dict\n    \n    def training_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)     \n        # logs metrics for each training_step, and the average across the epoch\n        self.log(\"training_loss\", loss)\n        for k,v in loss_dict.items():\n            self.log(\"train_\" + k, v.item())\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)     \n        self.log(\"validation/loss\", loss)\n        for k, v in loss_dict.items():\n            self.log(\"validation_\" + k, v.item())\n            \n        return loss\n\n    def configure_optimizers(self):\n        # DETR authors decided to use different learning rate for backbone\n        # you can learn more about it here: \n        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L22-L23\n        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L131-L139\n        param_dicts = [\n            {\n                \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n            {\n                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n                \"lr\": self.lr_backbone,\n            },\n        ]\n        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n\n    def train_dataloader(self):\n        return TRAIN_DATALOADER\n\n    def val_dataloader(self):\n        return VAL_DATALOADER","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir lightning_logs/\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n\nbatch = next(iter(TRAIN_DATALOADER))\noutputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs.logits.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning import Trainer\n\n\n# settings\nMAX_EPOCHS = 1\n\n# pytorch_lightning < 2.0.0\n# trainer = Trainer(gpus=1, max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n\n# pytorch_lightning >= 2.0.0\ntrainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n\ntrainer.fit(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path1 = \"/kaggle/working/IGG_Binary_Aperio_DETR.pt\"\ntorch.save(model, path1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}